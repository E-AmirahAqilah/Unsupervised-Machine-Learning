{"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ljM37B0Lbz8d"},"source":["<img src=\"images/cads-logo.png\" style=\"height: 100px;\" align=left> <img src=\"images/sklearn-logo.png\" style=\"height: 100px;\" align=right>\n","# Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"8jTrdaZabz8i"},"source":["# Day4: Clustering\n","**Clustering** is the task of partitioning the dataset into groups called clusters. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to classification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. The only difference here is that: The cluster \"number\" is not indicative of anything meaningful, or labels such as iris flower species, or breast cancer diagnosis or celebrity faces. The numbers only indicate groupings.\n","\n","![](https://www.analyticsvidhya.com/wp-content/uploads/2016/11/clustering.png)\n","\n","How is this useful? Well, clustering can discover the patterns that exist in your data in an exploratory setting. In terms of evaluation, there exists a few quantitative methods (which we will look later), but they mostly made assumptions that are not applicable in some applications. As such, the \"goodnesss\" of clustering is often hard to assess."]},{"cell_type":"markdown","metadata":{"id":"YQYprS6pbz8k"},"source":["## Table of Contents\n","\n","- 3 popular clustering algorithms\n","    - K-Means Clustering\n","        - Exercise 1\n","        - Elbow Method\n","        - Example: k-means on digits\n","        - Failure cases of k-means\n","        - Exercise: K-means on the Iris dataset\n","    - Agglomerative Clustering\n","        - Hierarchical clustering and dendrogram\n","            - Dendrogram Truncation\n","            - Interpreting the dendrogram further\n","            - Exercise 2\n","        - Cophenetic Correlation Coefficient\n","        - Exercise 3\n","        - Exercise 4\n","    - DBSCAN\n","- Comparing and Evaluating Clustering Algorithms\n","    - Clustering Evaluation Metrics (True label known)\n","        - Adjusted Rand Index\n","        - v_measure\n","    - Clustering Evaluation Metrics (True label unknown)\n","        - Silhouette Coefficient\n","        - Davies-Bouldin Index\n","- Summary\n","- Exercise 5\n","- Exercise 6\n","- Exercise 7"]},{"cell_type":"markdown","metadata":{"id":"9awQlK4Ybz8m"},"source":[" # 3 popular clustering algorithms\n","\n"," - **K-Means Clustering**\n"," -  **Agglomerative Hierarchical Clustering**\n"," -  **Density-Based Spatial Clustering of Applications with Noise (DBSCAN)**"]},{"cell_type":"markdown","metadata":{"id":"wtHIfFR_bz8o"},"source":["We begin with the standard imports:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5FixRbWbz8p"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaRZCQk4bz8r"},"outputs":[],"source":["from sklearn.datasets import make_blobs\n","from sklearn.datasets import make_moons\n","import plottinglib as plotlib"]},{"cell_type":"markdown","metadata":{"id":"xU8VuOLsbz8t"},"source":["## K-Means Clustering\n","\n","*K-Means clustering* is one of the simplest and most commonly used clustering algorithms.\n","It tries to find cluster centers that are representative of certain regions of the\n","data. The algorithm alternates between two steps: assigning each data point to the\n","closest cluster center, and then setting each cluster center as the mean of the data\n","points that are assigned to it. The algorithm iterates until the assignment of\n","instances to clusters no longer changes the cluster means by much. The following figure shows a simple step-by-step example of clustering.\n","\n","![](images/kmeans-steps.png)\n","\n","Cluster centers are shown as triangles, while data points are shown as circles. Colors indicate cluster membership. We specified that we are looking for three clusters, so the algorithm was initialized by declaring three data points *randomly* as cluster centers (see \"Initialization\"). Then the iterative algorithm starts.\n","- First, each data point is assigned to the cluster center it is closest to (see \"Assign Points (1)\").\n","- Next, the cluster centers are updated to be the mean of the assigned points (see \"Recompute Centers (1)\").\n","\n","Then the process is repeated two more times. After the third iteration, the assignment of points to cluster centers remained unchanged, so the algorithm stops.\n","\n","After clustering is performed, given new data points, k-means will assign each point to the closest cluster center. This process is sometimes known as \"quantization\" or \"vector quantization\".\n","\n","Applying k-means with `scikit-learn` is quite simple. We shall start off with some synthetic data before venturing into more realistic data. Similar to the classification estimators, the `KMeans` class is instantiated, and the number of clusters we are looking for are set. Then we call the `fit` method to perform the fitting to the data."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"bV_H5KYmbz8v"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","# generate synthetic two-dimensional data\n","X, y_true = make_blobs(n_samples=300, centers=4,\n","                       cluster_std=0.60, random_state=0)\n","\n","# build the clustering model\n","kmeans = KMeans(n_clusters=4)\n","kmeans.fit(X)"]},{"cell_type":"markdown","metadata":{"id":"aSjLd3g3bz8x"},"source":["During the algorithm, each training data point in X is assigned a cluster label. You can find these labels in the **kmeans.labels_** attribute."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlRwqmWKbz8z"},"outputs":[],"source":["print(\"Cluster memberships:\\n{}\".format(kmeans.labels_))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQAcaVIIbz81"},"outputs":[],"source":["kmeans.cluster_centers_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5ubqiO9bz82"},"outputs":[],"source":["# The inertia is the sum of squared distances of samples to their closest cluster center.\n","kmeans.inertia_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiUrOAagbz83"},"outputs":[],"source":["# plot data\n","colormap='jet'\n","plotlib.plot_estimator_boundaries(kmeans, X, y, cm=colormap, alpha=0.4, title='K-means boundaries')\n","plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, marker='o', cmap=colormap, alpha=0.6, edgecolors='grey')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c=[0, 1, 2, 3], marker='^',\n","            linewidths=4, cmap=colormap)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"oIEO_FMVbz84"},"source":["You can also assign cluster labels to new points, using the predict method. Each new point is assigned to the closest cluster center when predicting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZ6mP3TFbz85"},"outputs":[],"source":["# if we plot a scatter plot with the X and the label from KMean,\n","# we get similar to the earlier plot with c=y\n","\n","plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pro91mafbz86"},"outputs":[],"source":["print(kmeans.predict([[-10,-5]]))"]},{"cell_type":"markdown","metadata":{"id":"dn52afE6bz88"},"source":["Obviously, this makes more sense if we have some new data points that we want to assign them to clusters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lz9JZH5rbz88"},"outputs":[],"source":["# create some random data points that are roughly in the range\n","new_data = np.random.randn(10, 2)*5\n","print(new_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2U_rXLX-bz89"},"outputs":[],"source":["print(kmeans.predict(new_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VksECTPubz8-"},"outputs":[],"source":["# given the boundary that KMean made earlier,\n","# the predicted clusters for the new data is shown here.\n","\n","colormap='jet'\n","plotlib.plot_estimator_boundaries(kmeans, X, y,\n","                                  cm=colormap, alpha=0.4,\n","                                  title='K-means boundaries')\n","\n","plt.scatter(X[:, 0], X[:, 1],\n","            c=kmeans.labels_, marker='o',\n","            cmap=colormap, alpha=0.6,\n","            edgecolors='grey')\n","\n","plt.scatter(kmeans.cluster_centers_[:, 0],\n","            kmeans.cluster_centers_[:, 1],\n","            c=[0, 1, 2, 3], marker='^',\n","            linewidths=4, cmap=colormap)\n","\n","plt.scatter(new_data[:,0],\n","            new_data[:,1],\n","            c=kmeans.predict(new_data),\n","            marker='x')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qewafKOZbz8-"},"source":["In k-means clustering, running the algorithm again might result in a different clusters (usually when *k* is large), or even different numbering of clusters, because of the random nature of the initialization of cluster centers, also known as \"seeds\"."]},{"cell_type":"markdown","metadata":{"id":"_xY6pShjbz8_"},"source":["## <font color=Green><b>Exercise 1:</b></font>\n","Try with lesser or more number of clusters (k). Observe the effect of k-means clustering with different k values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXnpAErRbz8_"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","metadata":{"id":"ECAg6qZrbz8_"},"source":["### Using the elbow method to find the optimal number of clusters\n","The Elbow Method heuristic is probably the most popular technique of finding the optimal number of clusters due to its simple explanation (amount of variance explained by number of clusters) coupled with a visual check. Basically, it tries to find the clustering step where the acceleration of distance growth is the biggest.\n","\n","The inertia is the sum of squared distances of samples to their closest cluster center.\n","$$\\sum_{i=1}^{k}{\\sum_{x \\in C_i}{(x-c_i)^2}}.$$ Where $x=(x_1,x_2, ..., x_n)$ is a datapoint in cluster $C_i$ with centriod $c_i=(c_{i1}, c_{i2}, ..., c_{in})$.\n","\n","\n","More information is [here](http://scikit-learn.org/stable/modules/clustering.html#k-means)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1fwh-E3bz9A"},"outputs":[],"source":["print('Distortion: %.2f' % kmeans.inertia_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02qVc27tbz9A"},"outputs":[],"source":["distortions = []\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters=i,\n","                random_state=0)\n","    kmeans.fit(X)\n","    distortions.append(kmeans.inertia_)\n","plt.plot(range(1, 11), distortions, marker='o')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('Distortion')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ExhIJF8rbz9A"},"source":["The good news is that the k-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye. But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly."]},{"cell_type":"markdown","metadata":{"id":"o5_iYlAibz9B"},"source":["#### Is the number of clusters suggeted by elbow, acceptable?\n","\n","- One known issue of \"elbow\" is the way it is defined:\n","    - you need at least a right and a left point, which implies that this method will never be able to tell you that all your data is in one single cluster only.\n","- If your data has many clusters of different variances, then this method might get the optimal number of clusters a little wrong (either 1 more or 1 less usually)."]},{"cell_type":"markdown","metadata":{"id":"Nsz6kL18bz9C"},"source":["### Example: k-means on digits\n","\n","To start, let's take a look at applying *k-means* on the same simple digits data. Here we will attempt to use *k-means* to try to identify similar digits *without using the original label information*; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any *a priori* label information.\n","\n","We will start by loading the digits and then finding the ``KMeans`` clusters.\n","Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Toezx6jZbz9D"},"outputs":[],"source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","digits.data.shape"]},{"cell_type":"markdown","metadata":{"id":"4PSSJTkVbz9D"},"source":["The clustering can be performed as we did before:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSQLWdZYbz9E"},"outputs":[],"source":["digit_km = KMeans(n_clusters=10, random_state=0)\n","clusters = digit_km.fit_predict(digits.data)\n","digit_km.cluster_centers_.shape"]},{"cell_type":"markdown","metadata":{"id":"0u451KBdbz9F"},"source":["The result is 10 clusters in 64 dimensions.\n","Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \"typical\" digit within the cluster.\n","Let's see what these cluster centers look like:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycDZFan4bz9G"},"outputs":[],"source":["fig, ax = plt.subplots(2, 5, figsize=(10, 3))\n","centers = digit_km.cluster_centers_.reshape(10, 8, 8)\n","for axi, center in zip(ax.flat, centers):\n","    axi.set(xticks=[], yticks=[])\n","    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"]},{"cell_type":"markdown","metadata":{"id":"HLH6MpU6bz9G"},"source":["We see that *even without the labels*, ``KMeans`` is able to find clusters whose centers are recognizable digits, with perhaps some exceptions.\n","\n","Because *k*-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted.\n","We can fix this by matching each learned cluster label with the true labels found in them:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"om2eHl5rbz9G"},"outputs":[],"source":["digits.target[clusters==0]"]},{"cell_type":"markdown","metadata":{"id":"dEaF4VbCbz9H"},"source":["### Failure cases of k-means\n","\n","- Even if you know the \"right\" number of clusters for a given dataset, k-means might not always be able to recover them. (Example 1)\n","- k-means also assumes that all clusters have the same \"diameter\" in some sense; it always draws the boundary between clusters to be exactly in the middle between the cluster centers. That can sometimes lead to surprising results. (Example 2)\n","- Each cluster is a convex shape in k-means method. As a result of this, k-means can only capture relatively simple shapes. (Example 3)"]},{"cell_type":"markdown","metadata":{"id":"T-GoxViTbz9I"},"source":["### Example 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sk6uQAoIbz9I"},"outputs":[],"source":["# generate some random cluster data\n","X_varied, y_varied = make_blobs(n_samples=200,\n","                                cluster_std=[1.0, 2.5, 0.5],\n","                                random_state=170)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdIaWQW1bz9J"},"outputs":[],"source":["plt.scatter(X_varied[:,0], X_varied[:,1], c=y_varied)\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcQiZrsbbz9K"},"outputs":[],"source":["# cluster the data into three clusters\n","y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\n","\n","# plot the cluster assignments and cluster centers\n","plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred, cmap='viridis')\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ixbuo32bz9L"},"outputs":[],"source":["# Draw the same result by usung `model.labels_`\n","\n","# Your code here"]},{"cell_type":"markdown","metadata":{"id":"44J3NC5Pbz9L"},"source":["One might have expected the dense region in the lower left to be the first cluster, the dense region in the upper right to be the second, and the less dense region in the center to be the third. Instead, both denser clusters have some points that are far\n","away from all the other points in these clusters that are near their centers. k-means also assumes that all directions are equally important for each cluster."]},{"cell_type":"markdown","metadata":{"id":"GN6Mc693bz9M"},"source":["### Example 2:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"028Sh51bbz9N"},"outputs":[],"source":["# generate some random cluster data\n","X, y = make_blobs(random_state=170, n_samples=600)\n","rng = np.random.RandomState(74)\n","\n","# transform the data to be stretched\n","transformation = rng.normal(size=(2, 2))\n","X = np.dot(X, transformation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEKG7eQNbz9N"},"outputs":[],"source":["plt.scatter(X[:,0], X[:,1], c=y)\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKRcKdkXbz9O"},"outputs":[],"source":["# cluster the data into three clusters\n","kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)\n","y_pred = kmeans.predict(X)\n","\n","# plot the cluster assignments and cluster centers\n","plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')\n","plt.scatter(kmeans.cluster_centers_[:, 0],\n","            kmeans.cluster_centers_[:, 1], marker='^',\n","            c='red', s=100, linewidth=4, cmap='viridis')\n","\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"O_LhAfKlbz9P"},"source":["Here, we have a two-dimensional dataset where there are three clearly separated parts in the data. However, these groups are stretched in a diagonal way. As k-means only considers the distance to the nearest cluster center and that clusters are often circular in shape, it can't handle this kind of data."]},{"cell_type":"markdown","metadata":{"id":"FUSF1QbCbz9Q"},"source":["### Example 3:\n","k-means also performs poorly if the clusters have more complex shapes, like the `two_moons` data we have encountered before"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWYvjk3Lbz9S"},"outputs":[],"source":["# generate synthetic two_moons data (with less noise this time)\n","from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=200, noise=0.05, random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLythIZHbz9T"},"outputs":[],"source":["plt.scatter(X[:,0], X[:,1], c=y)\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pExNkoaNbz9T"},"outputs":[],"source":["# cluster the data into two clusters\n","kmeans = KMeans(n_clusters=2)\n","kmeans.fit(X)\n","y_pred = kmeans.predict(X)\n","\n","# plot the cluster assignments and cluster centers\n","plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='jet', s=60)\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='^', c=['blue', 'red'], s=100, linewidth=2)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RAZ_sKBUbz9V"},"source":["Opps, this doesn't look right. We have hoped that the clustering algorithm can discover the two half-moon shapes, but looks like it failed."]},{"cell_type":"markdown","metadata":{"id":"PLQif9Eobz9W"},"source":["## <font color=Green><b>Exercise: K-means on the Iris dataset</b></font>\n","\n","- Load the iris data set using sklearn datasets\n","- Set X to contain the feature matrix\n","- Set y to contain the target vector\n","- Scale data with StandardScaler and MinMaxScaler\n","- Fit a Kmeans Clustering model with 3 clusters for scaling and unscaling data\n","- Do we really see three flower groups here or should there be more/less? Plot using the elbow method\n","- Does scaling the data improve the performance of your clustering? (show with plotting)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVnNO-LZbz9W"},"outputs":[],"source":["from sklearn import datasets\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHxL0DJ5bz9X"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"pK0_pVrqbz9X"},"source":["# Agglomerative Clustering\n","\n","**Agglomerative clustering** refers to a collection of clustering algorithms that all build upon the same principles: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satisfied. The stopping criterion implemented in `scikit-learn` is the number of clusters, so similar clusters are merged until only the specified number of clusters are left. There are several linkage criteria that specify how exactly the \"most similar cluster\" is measured. This measure is always defined between two existing clusters.\n","The following three choices are implemented in scikit-learn:\n","* *ward*: The default choice, ward picks the two clusters to merge such that the variance\n","within all clusters increases the least. This often leads to clusters that are relatively\n","equally sized.\n","* *average*: Average linkage merges the two clusters that have the smallest average distance\n","between all their points.\n","* *complete*: Complete linkage (also known as maximum linkage) merges the two clusters that\n","have the smallest maximum distance between their points.\n","\n","*ward* works on most datasets, and we will use it in our examples. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), *average* or *complete* might work better. The following are some commonly used cluster distances, and their respective expressions:\n","\n","![](images/cluster-distances.png)\n","\n","\n","The following figure illustrates the progression of agglomerative clustering on a two-dimensional dataset, looking for three clusters:\n","\n","![](images/agglomerative-steps.png)\n","\n","Initially, each point is its own cluster. Then, in each step, the two clusters that are closest are merged. In the first four steps, two single-point clusters are picked and these are joined into two-point clusters. In step 5, one of the two-point clusters is extended to a third point, and so on. In step 9, there are only three clusters remaining. As we specified that we are looking for three clusters, the algorithm then stops. Let's have a look at how agglomerative clustering performs on the simple three cluster data we used here. Because of the way the algorithm works, **agglomerative clustering cannot make predictions for new data points**. Therefore, `AgglomerativeClustering` has no `predict` method. To build the model and get the cluster memberships on the training set, **use the `fit_predict` method** instead."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNFhT4Eabz9Y"},"outputs":[],"source":["X, y = make_blobs(random_state=1)\n","\n","plt.scatter(X[:, 0], X[:, 1], c='blue')\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQPPKHM2bz9a"},"outputs":[],"source":["from sklearn.cluster import AgglomerativeClustering\n","\n","agg = AgglomerativeClustering(n_clusters=3)\n","assignment = agg.fit_predict(X)\n","\n","plt.scatter(X[:, 0], X[:, 1], c=assignment, cmap='jet')\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"TBiFgJXKbz9a"},"source":["As expected, we get the same results here with such a simple dataset. While the `scikit-learn`\n","implementation of agglomerative clustering requires you to specify the number of\n","clusters you want the algorithm to find, agglomerative clustering methods provide\n","some help with choosing the right number, which we will come to next.\n","\n","### Hierarchical clustering and dendrogram\n","\n","Agglomerative clustering produces what is known as a *hierarchical clustering*. The clustering proceeds iteratively, and every point makes a journey from being a single point cluster to belonging to some final cluster. Each intermediate step provides a\n","clustering of the data (with a different number of clusters). The figure below shows an example of some data points which have been clustered in a hierarchical way, providing some insight into how each cluster breaks up into smaller clusters:\n","<img src=\"images/hierarchical-clusters.png\" width=\"400\" />"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"LpJAD276bz9b"},"source":["Visualizing this way is somewhat limited to only 2-D data. There is another way to visualize hierarchical clustering, called a *dendrogram*, that can handle multi-dimensional datasets. A **dendrogram** is a visualization in form of a tree showing the order and distances of merges during the hierarchical clustering.\n","\n","`scikit-learn` does not have the functionality to draw the dendrogram, but we can go to the SciPy library to generate it. Clustering algorithms in SciPy are slightly different in terms of usage. SciPy provides a function that takes a data array X and computes a *linkage array*, which encodes hierarchical cluster similarities. We can then feed this linkage array into the SciPy `dendrogram` function to plot the dendrogram."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jcke-oOJbz9b"},"outputs":[],"source":["from scipy.cluster.hierarchy import ward    #single, complete, average, weighted, centroid, median\n","\n","X, y = make_blobs(random_state=0, n_samples=12)\n","\n","# ward function returns an array that specifies the distances between clusters\n","linkage_array = ward(X)\n","print(linkage_array)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNQsE7dCbz9c"},"outputs":[],"source":["print(linkage_array[0])   # the first iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RW2Wnlaabz9e"},"outputs":[],"source":["print(linkage_array[:10])   # the first 10 iterations"]},{"cell_type":"markdown","metadata":{"id":"cWBSer_Kbz9e"},"source":["We can observe that until iteration 4 the algorithm only directly merged original samples. We can also observe the monotonic increase of the distance.\n","\n","\n","In iteration 4 the algorithm decided to merge cluster indices 5 with 15.\n","**Q:** What is index 15?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLyhOuLIbz9f"},"outputs":[],"source":["print(linkage_array[-4:,2])"]},{"cell_type":"markdown","metadata":{"id":"meyXe-dtbz9f"},"source":["Such distance jumps / gaps in the linkage_array are pretty interesting for us. They indicate that maybe the things that were merged here really don't belong to the same cluster."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"bpBjLk8Vbz9g"},"outputs":[],"source":["# plot the dendrogram\n","from scipy.cluster.hierarchy import dendrogram\n","\n","dendrogram(linkage_array)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cL7B457Bbz9g"},"source":["The dendrogram shows data points as points on the bottom (numbered from 0 to 11). Then, a tree is plotted with these points (representing single-point clusters) as the leaves, and a new parent node is added for each two clusters that are joined. At the top level, there are two branches, one consisting of points 5, 0, 11, 10, 7, 6, and 9, and the other consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clusters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j33cCRqsbz9h"},"outputs":[],"source":["plt.title('Hierarchical Clustering Dendrogram')\n","plt.xlabel('sample index')\n","plt.ylabel('distance')\n","\n","dendrogram(\n","    linkage_array,\n","    leaf_rotation=90,  # rotates the x axis labels\n","    leaf_font_size=8,  # font size for the x axis labels\n",")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lm8FEtBpbz9h"},"outputs":[],"source":["# generate two clusters: a with 100 points, b with 50:\n","np.random.seed(4711)  # for repeatability of this tutorial\n","a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n","b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n","X1 = np.concatenate((a, b),)\n","\n","print(X1.shape)\n","\n","plt.scatter(X1[:,0], X1[:,1])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SL9wRXAGbz9i"},"outputs":[],"source":["Z1 = ward(X1)\n","dendrogram(Z1)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Th9Oi5Iebz9j"},"source":["#### Dendrogram Truncation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUTnFOq7bz9k"},"outputs":[],"source":["plt.title('Hierarchical Clustering Dendrogram (truncated)')\n","plt.xlabel('sample index')\n","plt.ylabel('distance')\n","\n","dendrogram(\n","    Z1,\n","    truncate_mode='lastp',  # show only the last p merged clusters\n","    p=12,                   # p is the number of last clusters merged\n","    leaf_rotation=90,\n","    leaf_font_size=12)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"frzKXKyxbz9l"},"source":["The above shows a truncated dendrogram, which only shows the last p=12 out of our 149 merges."]},{"cell_type":"markdown","metadata":{"id":"JdtvH4lRbz9l"},"source":["### Interpreting the dendrogram further\n","\n","The y-axis in the dendrogram doesn't just specify when in the agglomerative algorithm two clusters get merged. The length of each branch also shows how far apart the merged clusters are."]},{"cell_type":"markdown","metadata":{"id":"hKh_eew_bz9m"},"source":["## <font color=Green><b>Exercise 2:</b></font>\n","For the following dataset use hierarchical clustering and compare the dendrograms for different linkage types."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"krPYpWC-bz9m"},"outputs":[],"source":["np.random.seed(4711)  # for repeatability of this tutorial\n","a = np.random.multivariate_normal([5, 0], [[3, 2], [1, 4]], size=[10,])\n","b = np.random.multivariate_normal([0, 10], [[3, 1], [1, 4]], size=[5,])\n","X2 = np.concatenate((a, b),)\n","\n","plt.scatter(X2[:,0], X2[:,1])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7FLzEuobz9m"},"outputs":[],"source":["# your code here"]},{"cell_type":"markdown","metadata":{"id":"51EULXQWbz9n"},"source":["### Cophenetic Correlation Coefficient\n","\n","One thing we can do to check if our choice of distance metric is good is to check the Cophenetic Correlation Coefficient of your clustering with help of the `cophenet()` function. This compares the actual pairwise distances of all your samples to those implied by the hierarchical clustering. **The closer the value is to 1, the better the clustering** preserves the original distances."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPdiqJDdbz9o"},"outputs":[],"source":["from scipy.cluster.hierarchy import cophenet\n","from scipy.spatial.distance import pdist\n","\n","# pdist function returns a distance matrix representing distance of all sample pairs\n","c, coph_dists = cophenet(linkage_array, pdist(X))\n","print(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soXdNU-Rbz9o"},"outputs":[],"source":["from scipy.cluster.hierarchy import single, complete, average\n","\n","Z = single(X)\n","c, coph_dists = cophenet(Z, pdist(X))\n","print(c)\n","\n","Z = complete(X)\n","c, coph_dists = cophenet(Z, pdist(X))\n","print(c)\n","\n","Z = average(X)\n","c, coph_dists = cophenet(Z, pdist(X))\n","print(c)"]},{"cell_type":"markdown","metadata":{"id":"sGWNbJr8bz9q"},"source":["So, average linkage may be a slightly better option compared the the ward linkage."]},{"cell_type":"markdown","metadata":{"id":"hxUiWfUzbz9q"},"source":["## <font color=Green><b>Exercise 3:</b></font>\n","Cluster the following dataset using hierarchical clustering with linkage: {ward, complete, average, single}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9s8VbEKSbz9q"},"outputs":[],"source":["X, y = make_moons(n_samples=200, noise=0.05, random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvnkY5Izbz9q"},"outputs":[],"source":["# your code here"]},{"cell_type":"markdown","metadata":{"id":"IoMSuCKcbz9s"},"source":["## <font color=Green><b>Exercise 4:</b></font>\n","We have investigated Failure cases of k-means in 3 examples. Apply Agglomerative clustering for the data have been used in those examples and see if the resulting clustering is a success."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVv2tc8hbz9s"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","metadata":{"id":"ZPULUUvRbz9t"},"source":["# Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n","\n","Another very useful clustering algorithm is **DBSCAN** (which stands for \"density based\n","spatial clustering of applications with noise\"). The main benefits of DBSCAN are that it does not require the user to set the number of clusters a priori, it can capture clusters of complex shapes, and it can identify points that are not part of any\n","cluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets.\n","\n","DBSCAN works by identifying points that are in \"crowded\" regions of the feature space, where many data points are close together. These regions are referred to as dense regions in feature space. The idea behind DBSCAN is that clusters form dense\n","regions of data, separated by regions that are relatively empty.\n","\n","Points that are within a dense region are called core samples (or core points). There are two parameters in DBSCAN: `min_samples` and `eps`. If there are at least `min_samples` number of data points within a distance of `eps` to a given\n","data point, that data point is classified as a core sample. Core samples that are closer to each other than the distance eps are put into the same cluster by DBSCAN.\n","\n","### How does this work?\n","\n","The algorithm works by picking an arbitrary point to start with. It then finds all points with distance `eps` or less from that point. If there are less than `min_samples` points within distance `eps` of the starting point, this point is labeled as \"noise\", meaning that it doesn't belong to any cluster. If there are more than `min_samples` points within a distance of `eps`, the point is labeled a \"core sample\" and assigned a new cluster label. Then, all neighbors (within `eps`) of the point are visited. If they have not been assigned a cluster yet, they are assigned the new cluster label that was just created. If\n","they are core samples, their neighbors are visited in turn, and so on. The cluster\n","grows until there are no more core samples within distance `eps` of the cluster. Then another point that hasn't yet been visited is picked, and the same procedure is repeated.\n","\n","Concisely, in DBSCAN, a special label is assigned to each sample (point) using the following criteria:\n"," - **Core Sample:** A point is considered as *core sample* if at least a specified number (`min_samples`) of neighboring points fall within the specified radius `eps`.\n"," - **Boundary Point**: It is a point that has fewer neighbors than `min_samples` within `eps`, but lies within the `eps` radius of a core point. Means that the boundary point itself doesn't meet the `min_samples` criteria.\n"," - **Noise**: All other points that are neither core nor border points are considered as *noise* points.\n","\n"," ![(image)](images/dbscan.png)\n","\n","DBSCAN does not allow predictions on new test data, so we need to `fit_predict` at the same time to perform clustering and return the cluster labels in one step:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14yRwwfQbz9u"},"outputs":[],"source":["X, y = make_blobs(random_state=0, n_samples=25)\n","\n","plt.scatter(X[:,0], X[:,1], c=\"blue\")\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qj8rGamGbz9v"},"outputs":[],"source":["from sklearn.cluster import DBSCAN\n","\n","dbscan = DBSCAN()\n","print(dbscan)\n","clusters = dbscan.fit_predict(X)\n","print(\"Cluster memberships:\\n{}\".format(clusters))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"mUFuDs7Tbz9v"},"source":["Opps! As you can see, all data points were assigned the label -1, which stands for noise. This is a consequence of the default parameter settings for eps and min_samples, which are not tuned for small toy datasets. Let's try other combination of settings then..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaDMlxHAbz9w"},"outputs":[],"source":["for ms in list([2, 3, 5]):\n","    for e in list([1, 1.5, 2, 3]):\n","        dbscan = DBSCAN(min_samples=ms, eps=e)\n","        clusters = dbscan.fit_predict(X)\n","        print(\"min samples: \", ms, \" eps: \", e, \" clusters: \", clusters)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"hsupSP0Ybz9w"},"source":["Let's take a closer look at some of these settings, particularly those that have clear cut clusters (cluster numbers 0, 1, 2), and those with some noise points.\n","\n","`DBSCAN` comes with some nice properties. **`core_sample_indices_`** gets us the indices of the core samples, whereas noise samples can be found by the label -1, what remains are the boundary samples. Let's create a plot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CiIjGZNbz9w"},"outputs":[],"source":["#Let's determine which data points are considered to be core_points, boundray_points and noise points\n","ms = 5\n","ep = 2\n","dbscan = DBSCAN(min_samples=ms, eps=ep)\n","clusters = dbscan.fit_predict(X)\n","print(clusters)\n","print(\"===============================\")\n","\n","all_samples = np.arange(0,len(X))\n","print(\"All samples: \",all_samples)\n","\n","core = dbscan.core_sample_indices_\n","print(\"Core samples: \", core)\n","\n","noise = np.where(clusters == -1)[0]       # which data points have -1 label\n","print(\"Noise: \", noise)\n","\n","boundary = np.setdiff1d(np.setdiff1d(all_samples, core_samples), noise)\n","print(\"Boundary samples: \", boundary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Es6VFNLVbz9x"},"outputs":[],"source":["# core_points, boundray_points and noise points\n","plt.scatter(X[core,0], X[core,1], c='g', label='Core')\n","plt.scatter(X[noise,0], X[noise,1], c='r', label='Noise')\n","plt.scatter(X[boundary,0], X[boundary,1], c='magenta', label='Boundary')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqCoLglXbz9z"},"outputs":[],"source":["# plot the cluster assignments\n","plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='jet', s=60)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()\n","\n","print(clusters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1nuoynQbz91"},"outputs":[],"source":["# plot all three types of points\n","fig, ax = plt.subplots()\n","plt.scatter(X[boundary,0], X[boundary,1],\n","            c='magenta', cmap='viridis',\n","             marker='*', s=60)\n","\n","plt.scatter(X[core,0], X[core,1],\n","            c='g', cmap='viridis',\n","             marker='^', s=120)\n","\n","plt.scatter(X[noise,0], X[noise,1],\n","            c='red', cmap='viridis',\n","            marker='o', s=40)\n","\n","# draw circles of radius eps around each core sample\n","for i in core:\n","    circle = plt.Circle((X[i, 0], X[i, 1]), ep, color='blue', Fill=False)\n","    ax.add_artist(circle)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e9KAhNYjbz91"},"source":["**Tip**: While DBSCAN doesn’t require setting the number of clusters explicitly, setting `eps` implicitly controls how many clusters will be found. Finding a good setting for `eps` is sometimes easier after scaling the data using `StandardScaler` or `MinMaxScaler` ([here](http://scikit-learn.org/stable/modules/preprocessing.html) is more information), as using these scaling techniques."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQvyhJp4bz93"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n","\n","# rescale the data to zero mean and unit variance\n","scaler = StandardScaler()\n","scaler.fit(X)\n","X_scaled = scaler.transform(X)\n","\n","dbscan = DBSCAN()\n","clusters = dbscan.fit_predict(X_scaled)\n","\n","# plot the cluster assignments\n","plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='jet', s=60)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OCGKb1B9bz94"},"source":["Perfect!\n","\n","One of the main advantages of using DBSCAN is that it does not assume that the clusters have a spherical shape as in k-means. Furthermore, DBSCAN is different from k-means and hierarchical clustering in that it doesn't necessarily assign each point to a cluster but is capable of removing noise points if they fall too far from significant clusters."]},{"cell_type":"markdown","metadata":{"id":"_fuS0whEbz94"},"source":["# Comparing and Evaluating Clustering Algorithms\n","\n","One of the challenges in applying clustering algorithms is that it is very hard to assess how well an algorithm worked, and to compare outcomes between different algorithms. After talking about the algorithms behind k-means, agglomerative clustering,\n","and DBSCAN, we will now compare them on some real-world datasets.\n","\n","There are metrics that can be used to assess the outcome of a clustering algorithm with knowing and unknowing the true labels of our datapoints.\n","\n","## Clustering Evaluation Metrics (True label known)\n","\n","### Adjusted Rand Index\n","Adjusted Rand Index measures the similarity between `y_true` and `y_pred`.\n","\n","To calculate similarity between two cluster assignments in `y_true` and `y_pred`, this metric calculates:\n","   - **a:** The number of pairs of elements that are in the same set in `y_true` and in the same set in `y_pred`\n","   - **b:** The number of pairs of elements that are in different sets in `y_true` and in different sets in `y_pred`\n","\n","$$Rand_{Index}=RI=\\frac{a+b}{{n\\choose 2}}$$, where n = number of datapoints = length of `y_true` </br>\n","\n","**Perfect labeling has RI = 1.0 and bad labeling has RI = 0.0.** </br>\n","\n","Adjusted Rand Index, adjusts the Rand Index such that random assignment of the labels results in Adjusted_Rand_Index=0. Adjusted Rand Index is 1.0 for perfecr labeling and zero or negative for bad labeling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEy5_Jugbz95"},"outputs":[],"source":["from sklearn.metrics import adjusted_rand_score\n","\n","adjusted_rand_score(digits.target, clusters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AErsnC0ibz95"},"outputs":[],"source":["adjusted_rand_score([2, 2, 1, 3], [0, 0, 4, 5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8yfzrE2bz96"},"outputs":[],"source":["adjusted_rand_score([2, 2, 1, 3], [0, 0, 4, 4])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ezwot0Ibz96"},"outputs":[],"source":["adjusted_rand_score([2, 2, 3, 3], [0, 0, 0, 0])\n","# adjusted_rand_score([1, 2, 3, 4], [0, 0, 0, 0])\n","# adjusted_rand_score([1, 1, 1, 3], [0, 0, 0, 0])"]},{"cell_type":"markdown","metadata":{"id":"e6wAUJngbz98"},"source":["### v_measure\n","To evaluate the goodness of the clustering v_measure uses the following metrics:\n","\n","   - **Homogeneity**: each cluster contains only members of a single class.\n","   - **Completeness**: all members of a given class are assigned to the same cluster.\n","\n","Then v_measure is the harmonic mean of them. $$v_{measure}=2\\times \\frac{h \\times c}{h + c}$$\n","\n","The v_measure score is between 0 and 1 and 1 is for perfectly complete labeling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2W9TjeUbz99"},"outputs":[],"source":["from sklearn.metrics import v_measure_score\n","\n","v = v_measure_score(digits.target, clusters)\n","print('v_measure:',v)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bs26CClIbz99"},"outputs":[],"source":["from sklearn.metrics import homogeneity_score, completeness_score\n","\n","h = homogeneity_score(digits.target, clusters)\n","print('homogeneity_score:', h)\n","\n","c = completeness_score(digits.target, clusters)\n","print('completeness_score:', c)\n","\n","v_manual = 2 * h * c / (h + c)\n","v == v_manual"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsMywPwQbz9-"},"outputs":[],"source":["v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n","# v_measure_score([2, 2, 1, 3], [0, 0, 4, 5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imKsO54-bz9-"},"outputs":[],"source":["v_measure_score([2, 2, 1, 3], [0, 0, 4, 4])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nh8-AZdbz9_"},"outputs":[],"source":["v_measure_score([2, 2, 1, 3], [0, 1, 4, 4])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ExCBpN1bz9_"},"outputs":[],"source":["v_measure_score([2, 2, 3, 3], [0, 0, 0, 0])"]},{"cell_type":"markdown","metadata":{"id":"DUV8BHPsbz-A"},"source":["## Clustering Evaluation Metrics (True label unknown)\n","\n","In the above example, we could use the confusion metrix or accuracy because we knew the true digit in each image however, clustering is an unsupervised algorithm that clusters (labels) datapoints only based on their features without knowing the real labeles of them. To evaluate the *goodness* of the resulting clusters, we need clustering evaluation metrics.\n","\n","To evaluate the goodness of a clustering model or comparing the performance of variuos clustering algorithms we need to evaluate the resulting clusters. Basically, a good clustering algorithm generates clusters with high intracluster homogeneity and good inter-cluster separation.\n","\n","### Silhouette Coefficient\n","Silhouette Coefficient evaluates how similar an object is to its own cluster (*cohesion*) compared to other clusters (*separation*).\n","\n","    - a: The mean distance between a sample and all other points in its cluster (*cohesion*).\n","    - b: The mean distance between a sample and all other points in the next nearest cluster (*separation*).\n","\n","Silhouette Coefficient = $\\frac{b-a}{max(a,b)}$\n","\n","    - This score is bounded between -1 and +1.\n","    - For highly-dense well-separated clusters a is close to zero and b is positive, therefore s is close to +1.\n","    - For incorrect clustering s is close to -1.\n","    - Scores around zero indicate overlapping clusters.\n","    \n","**Attention**: We mainly use Silhouette Coefficien to find the best number of the clusters similar to the elbow method.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AqTWVVVVbz-B"},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n","\n","s = silhouette_score(digits.data, clusters, metric = 'euclidean')\n","print('silhouette_score:', s)"]},{"cell_type":"markdown","metadata":{"id":"1KQZ3fgZbz-B"},"source":["### Davies-Bouldin Index\n","Davies-Bouldin Index method evaluates the average similarity between each cluster $C_i$ for $i=1, 2, \\dots, k$ and its most similar one $C_j$.\n"," - $s_i$: the average distance between each point of cluster i and the centroid of that cluster – also know as cluster diameter.\n"," - $d_{ij}$:the distance between cluster centroids $C_i$ and $C_j$.\n"," $$R_{ij}=\\frac{s_i+s_j}{d_ij}$$\n","\n"," Davies-Bouldin index= DB=$\\frac{1}{k} \\sum_{i=1}^{k} {max(R_{ij})}$\n","\n"," **Attention:** For good clustering, this value is close to zero."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wx5wuXYQbz-B"},"outputs":[],"source":["from sklearn.metrics import davies_bouldin_score\n","\n","db = davies_bouldin_score(digits.data, clusters)\n","print('davies_bouldin_score: ', db)"]},{"cell_type":"markdown","metadata":{"id":"w6xaUZj2bz-C"},"source":["## Summary\n","\n","This section has shown that applying and evaluating clustering is a highly qualitative\n","procedure, and often most helpful in the exploratory phase of data analysis. We\n","looked at three clustering algorithms: **k-means, DBSCAN,** and **agglomerative clustering**.\n","All three have a way of controlling the granularity of clustering. k-means and\n","agglomerative clustering allow you to specify the number of desired clusters, while\n","DBSCAN lets you define proximity using the eps parameter, which indirectly influences\n","cluster size. All three methods can be used on large, real-world datasets, are relatively\n","easy to understand, and allow for clustering into many clusters.\n","\n","Strengths of each of these algorithms:\n","* **k-means**: allows characterization of clusters using cluster means (top-down). It can also be viewed as a decomposition method by way of vector quantization.\n","* **agglomerative**: provides whole hierarchy of possible partitions of the data (bottom-up), which can be inspected via dendrograms\n","* **DBSCAN**: allows for detection of outlier points that are not part of any cluster, and it can help determine automatically the number of clusters. It also allows for more complex shapes (just like the `two_moons` data) to be clustered correctly."]},{"cell_type":"markdown","metadata":{"id":"GaLGturtbz-D"},"source":["## <font color=Green><b>Exercise 5:</b></font>\n","For the given data, build K-means and Agglomerative clustering models.\n","Using the Elbow method find the best number of clusters, evaluate them and then compare the results for selecting the best one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJmE1ajIbz-D"},"outputs":[],"source":["X, y_true = make_blobs(n_samples=300, centers=4,\n","                       cluster_std=[1, 1, 0.5, 0.4], random_state=0)\n","\n","plt.scatter(X[:, 0], X[:, 1], s=50)\n","\n","# Your code here"]},{"cell_type":"markdown","metadata":{"id":"zuitof8ybz-D"},"source":["## <font color=Green><b>Exercise 6:</b></font>\n","For the given data, Using the DBSCAN method, build a model and find the best values for min-sample and eps arguments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eflK6mKlbz-E"},"outputs":[],"source":["X, y = make_blobs(random_state=170, n_samples=600)\n","rng = np.random.RandomState(74)\n","transformation = rng.normal(size=(2, 2))\n","X = np.dot(X, transformation)\n","\n","# Your code here"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"UawmnXjXbz-F"},"source":["## <font color=Green><b>Exercise 7:</b></font>\n","Cluster iris dataset using hierarchical clustering with linkage: {ward, complete, average, single}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7UzXgIIbz-G"},"outputs":[],"source":["# get Iris\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kryxyN5dbz-H"},"outputs":[],"source":["# Scaling data\n","\n","# Your code here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEBOzEbWbz-I"},"outputs":[],"source":["# Your code here"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}